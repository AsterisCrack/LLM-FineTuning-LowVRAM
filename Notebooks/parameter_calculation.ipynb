{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculation of trainable parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import LoraConfig\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    AutoTokenizer\n",
    ")\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['bnb_8bit_quant_type', 'bnb_8bit_use_double_quant', 'bnb_8bit_compute_dtype']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model Qwen/Qwen2.5-7B...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6d90edca94548aa84ed47599ccafb44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(152064, 3584)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2SdpaAttention(\n",
       "          (q_proj): Linear8bitLt(in_features=3584, out_features=3584, bias=True)\n",
       "          (k_proj): Linear8bitLt(in_features=3584, out_features=512, bias=True)\n",
       "          (v_proj): Linear8bitLt(in_features=3584, out_features=512, bias=True)\n",
       "          (o_proj): Linear8bitLt(in_features=3584, out_features=3584, bias=False)\n",
       "          (rotary_emb): Qwen2RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear8bitLt(in_features=3584, out_features=18944, bias=False)\n",
       "          (up_proj): Linear8bitLt(in_features=3584, out_features=18944, bias=False)\n",
       "          (down_proj): Linear8bitLt(in_features=18944, out_features=3584, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3584, out_features=152064, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# Model name\n",
    "model_name = \"Qwen/Qwen2.5-7B\"\n",
    "\n",
    "# Load the pre-trained model\n",
    "print(f\"Loading model {model_name}...\")\n",
    "# Config for 4 bit quantization\n",
    "nf4_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Config for 8 bit quantization\n",
    "nf8_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    bnb_8bit_quant_type=\"nf8\",\n",
    "    bnb_8bit_use_double_quant=True,\n",
    "    bnb_8bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    torch_dtype=torch.float16,  # Use float16 for mixed precision training\n",
    "    device_map=\"auto\",  # Distribute the model automatically across GPUs\n",
    "    # quantization_config=nf4_config,  # Use the bitsandbytes quantization NF4 config\n",
    "    quantization_config=nf8_config,  # Use the bitsandbytes quantization NF8 config\n",
    ")\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    lora_alpha=32,  # Scaling factor for LoRA updates\n",
    "    lora_dropout=0.05,  # Dropout rate applied to LoRA layers\n",
    "    r=64,  # Rank of the LoRA decomposition\n",
    "    bias=\"none\",  # No bias is added to the LoRA layers\n",
    "    task_type=\"CAUSAL_LM\",  # Specify the task as causal language modeling\n",
    "    target_modules=[  # Modules to apply LoRA to\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our base model is structured like this: \\\n",
    "Qwen2ForCausalLM(\n",
    "  (model): Qwen2Model( \\\n",
    "    (embed_tokens): Embedding(152064, 3584) \\\n",
    "    (layers): ModuleList( \\\n",
    "      (0-27): 28 x Qwen2DecoderLayer( \\\n",
    "        (self_attn): Qwen2SdpaAttention( \\\n",
    "          (q_proj): Linear8bitLt(in_features=3584, out_features=3584, bias=True) \\\n",
    "          (k_proj): Linear8bitLt(in_features=3584, out_features=512, bias=True) \\\n",
    "          (v_proj): Linear8bitLt(in_features=3584, out_features=512, bias=True) \\\n",
    "          (o_proj): Linear8bitLt(in_features=3584, out_features=3584, bias=False) \\\n",
    "          (rotary_emb): Qwen2RotaryEmbedding() \\\n",
    "        ) \\\n",
    "        (mlp): Qwen2MLP( \\\n",
    "          (gate_proj): Linear8bitLt(in_features=3584, out_features=18944, bias=False) \\\n",
    "          (up_proj): Linear8bitLt(in_features=3584, out_features=18944, bias=False) \\\n",
    "          (down_proj): Linear8bitLt(in_features=18944, out_features=3584, bias=False) \\\n",
    "          (act_fn): SiLU() \\\n",
    "        ) \\\n",
    "        (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06) \\\n",
    "        (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06) \\\n",
    "      ) \\\n",
    "    ) \\\n",
    "    (norm): Qwen2RMSNorm((3584,), eps=1e-06) \\\n",
    "  ) \\\n",
    "  (lm_head): Linear(in_features=3584, out_features=152064, bias=False) \\\n",
    ")\n",
    "\n",
    "We are training the following layers:\n",
    "- q_proj\n",
    "- k_proj\n",
    "- v_proj\n",
    "- o_proj\n",
    "- gate_proj\n",
    "- up_proj\n",
    "- down_proj\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate the number of parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_features = 3584\n",
    "q_o_out_features = 3584\n",
    "k_v_out_features = 512\n",
    "gate_up_out_features = 18944\n",
    "down_in_features = 18944\n",
    "down_out_features = 3584\n",
    "\n",
    "r = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LoRa applied to a m x n matrix constists on creating A and B matrices such that A dim = m x r and B dim = r x n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_proj_params = in_features * r + r * q_o_out_features\n",
    "k_proj_params = in_features * r + r * k_v_out_features\n",
    "v_proj_params = in_features * r + r * k_v_out_features\n",
    "o_proj_params = q_o_out_features * r + r * in_features\n",
    "gate_proj_params = in_features * r + r * gate_up_out_features\n",
    "up_proj_params = in_features * r + r * gate_up_out_features\n",
    "down_proj_params = down_in_features * r + r * down_out_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating LoRA paremeters for rank 64...\n",
      "q_proj: 3584 x 64 + 64 x 3584 = 458752\n",
      "k_proj: 3584 x 64 + 64 x 512 = 262144\n",
      "v_proj: 3584 x 64 + 64 x 512 = 262144\n",
      "o_proj: 3584 x 64 + 64 x 3584 = 458752\n",
      "gate_proj: 3584 x 64 + 64 x 18944 = 1441792\n",
      "up_proj: 3584 x 64 + 64 x 18944 = 1441792\n",
      "down_proj: 18944 x 64 + 64 x 3584 = 1441792\n",
      "\n",
      "This transformer layer is applied 28 times in the model, so the total number of parameters is:\n",
      "161480704\n"
     ]
    }
   ],
   "source": [
    "print(f\"Calculating LoRA paremeters for rank {r}...\")\n",
    "print(f\"q_proj: {in_features} x {r} + {r} x {q_o_out_features} = {q_proj_params}\")\n",
    "print(f\"k_proj: {in_features} x {r} + {r} x {k_v_out_features} = {k_proj_params}\")\n",
    "print(f\"v_proj: {in_features} x {r} + {r} x {k_v_out_features} = {v_proj_params}\")\n",
    "print(f\"o_proj: {q_o_out_features} x {r} + {r} x {in_features} = {o_proj_params}\")\n",
    "print(f\"gate_proj: {in_features} x {r} + {r} x {gate_up_out_features} = {gate_proj_params}\")\n",
    "print(f\"up_proj: {in_features} x {r} + {r} x {gate_up_out_features} = {up_proj_params}\")\n",
    "print(f\"down_proj: {down_in_features} x {r} + {r} x {down_out_features} = {down_proj_params}\")\n",
    "print()\n",
    "print(f\"This transformer layer is applied 28 times in the model, so the total number of parameters is:\")\n",
    "total_params = 28 * (q_proj_params + k_proj_params + v_proj_params + o_proj_params + gate_proj_params + up_proj_params + down_proj_params)\n",
    "print(f\"{total_params}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
