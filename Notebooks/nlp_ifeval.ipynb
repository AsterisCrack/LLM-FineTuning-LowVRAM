{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: absl-py in /home/asteris/.local/lib/python3.10/site-packages (2.1.0)\n",
      "Requirement already satisfied: langdetect in /home/asteris/.local/lib/python3.10/site-packages (1.0.9)\n",
      "Requirement already satisfied: nltk in /home/asteris/.local/lib/python3.10/site-packages (3.9.1)\n",
      "Requirement already satisfied: immutabledict in /home/asteris/.local/lib/python3.10/site-packages (4.2.1)\n",
      "Requirement already satisfied: datasets in /home/asteris/.local/lib/python3.10/site-packages (2.19.1)\n",
      "Requirement already satisfied: bitsandbytes in /home/asteris/.local/lib/python3.10/site-packages (0.44.1)\n",
      "Requirement already satisfied: six in /home/asteris/.local/lib/python3.10/site-packages (from langdetect) (1.16.0)\n",
      "Requirement already satisfied: click in /home/asteris/.local/lib/python3.10/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /home/asteris/.local/lib/python3.10/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/asteris/.local/lib/python3.10/site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in /home/asteris/.local/lib/python3.10/site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: filelock in /home/asteris/.local/lib/python3.10/site-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/asteris/.local/lib/python3.10/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /home/asteris/.local/lib/python3.10/site-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /home/asteris/.local/lib/python3.10/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/asteris/.local/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/asteris/.local/lib/python3.10/site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/asteris/.local/lib/python3.10/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: xxhash in /home/asteris/.local/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /home/asteris/.local/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.3.1,>=2023.1.0 in /home/asteris/.local/lib/python3.10/site-packages (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets) (2024.3.1)\n",
      "Requirement already satisfied: aiohttp in /home/asteris/.local/lib/python3.10/site-packages (from datasets) (3.10.10)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in /home/asteris/.local/lib/python3.10/site-packages (from datasets) (0.24.6)\n",
      "Requirement already satisfied: packaging in /home/asteris/.local/lib/python3.10/site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/asteris/.local/lib/python3.10/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: torch in /home/asteris/.local/lib/python3.10/site-packages (from bitsandbytes) (2.5.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/asteris/.local/lib/python3.10/site-packages (from aiohttp->datasets) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/asteris/.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/asteris/.local/lib/python3.10/site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/asteris/.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/asteris/.local/lib/python3.10/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /home/asteris/.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.16.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/asteris/.local/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/asteris/.local/lib/python3.10/site-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/asteris/.local/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/asteris/.local/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/asteris/.local/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/asteris/.local/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/asteris/.local/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/asteris/.local/lib/python3.10/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/asteris/.local/lib/python3.10/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: networkx in /home/asteris/.local/lib/python3.10/site-packages (from torch->bitsandbytes) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/asteris/.local/lib/python3.10/site-packages (from torch->bitsandbytes) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/asteris/.local/lib/python3.10/site-packages (from torch->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/asteris/.local/lib/python3.10/site-packages (from torch->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/asteris/.local/lib/python3.10/site-packages (from torch->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/asteris/.local/lib/python3.10/site-packages (from torch->bitsandbytes) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/asteris/.local/lib/python3.10/site-packages (from torch->bitsandbytes) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/asteris/.local/lib/python3.10/site-packages (from torch->bitsandbytes) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/asteris/.local/lib/python3.10/site-packages (from torch->bitsandbytes) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/asteris/.local/lib/python3.10/site-packages (from torch->bitsandbytes) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/asteris/.local/lib/python3.10/site-packages (from torch->bitsandbytes) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/asteris/.local/lib/python3.10/site-packages (from torch->bitsandbytes) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/asteris/.local/lib/python3.10/site-packages (from torch->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/asteris/.local/lib/python3.10/site-packages (from torch->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /home/asteris/.local/lib/python3.10/site-packages (from torch->bitsandbytes) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/asteris/.local/lib/python3.10/site-packages (from torch->bitsandbytes) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/asteris/.local/lib/python3.10/site-packages (from sympy==1.13.1->torch->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/asteris/.local/lib/python3.10/site-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/asteris/.local/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes) (3.0.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install absl-py langdetect nltk immutabledict datasets bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['bnb_8bit_quant_type', 'bnb_8bit_use_double_quant', 'bnb_8bit_compute_dtype']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3e708a61a05409789d251b6805e250d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:   0%|          | 0/68 [00:00<?, ?batch/s]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "2024-11-23 11:47:04.927501: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-23 11:47:05.107612: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-23 11:47:05.176763: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-23 11:47:05.195647: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-23 11:47:05.330571: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-23 11:47:06.178483: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Processing Batches:   1%|▏         | 1/68 [02:16<2:32:48, 136.85s/batch]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Processing Batches:   3%|▎         | 2/68 [04:19<2:21:11, 128.35s/batch]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Processing Batches:   4%|▍         | 3/68 [06:42<2:26:39, 135.38s/batch]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Processing Batches:   6%|▌         | 4/68 [09:12<2:30:13, 140.83s/batch]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Processing Batches:   7%|▋         | 5/68 [11:20<2:23:15, 136.43s/batch]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Processing Batches:   9%|▉         | 6/68 [13:39<2:21:47, 137.22s/batch]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Processing Batches:  10%|█         | 7/68 [15:57<2:19:36, 137.32s/batch]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Processing Batches:  12%|█▏        | 8/68 [18:02<2:13:38, 133.64s/batch]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Processing Batches:  13%|█▎        | 9/68 [20:17<2:11:49, 134.07s/batch]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Processing Batches:  15%|█▍        | 10/68 [22:35<2:10:45, 135.28s/batch]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Processing Batches:  16%|█▌        | 11/68 [24:54<2:09:21, 136.16s/batch]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Processing Batches:  18%|█▊        | 12/68 [27:18<2:09:19, 138.57s/batch]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Processing Batches:  19%|█▉        | 13/68 [29:25<2:03:58, 135.24s/batch]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Processing Batches:  21%|██        | 14/68 [32:19<2:12:18, 147.00s/batch]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Processing Batches:  22%|██▏       | 15/68 [34:30<2:05:34, 142.15s/batch]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Processing Batches:  24%|██▎       | 16/68 [36:47<2:01:40, 140.39s/batch]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Processing Batches:  25%|██▌       | 17/68 [39:03<1:58:18, 139.18s/batch]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Processing Batches:  26%|██▋       | 18/68 [41:27<1:57:05, 140.50s/batch]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Processing Batches:  28%|██▊       | 19/68 [43:49<1:55:19, 141.22s/batch]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Processing Batches:  29%|██▉       | 20/68 [45:59<1:50:17, 137.87s/batch]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Processing Batches:  31%|███       | 21/68 [48:14<1:47:15, 136.92s/batch]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Processing Batches:  32%|███▏      | 22/68 [50:33<1:45:27, 137.55s/batch]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Processing Batches:  34%|███▍      | 23/68 [52:42<1:41:17, 135.06s/batch]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Processing Batches:  35%|███▌      | 24/68 [55:55<1:51:41, 152.30s/batch]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Processing Batches:  37%|███▋      | 25/68 [58:00<1:43:11, 143.98s/batch]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Processing Batches:  38%|███▊      | 26/68 [1:00:06<1:37:10, 138.83s/batch]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Processing Batches:  40%|███▉      | 27/68 [1:02:20<1:33:45, 137.20s/batch]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Processing Batches:  41%|████      | 28/68 [1:04:32<1:30:27, 135.70s/batch]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Processing Batches:  43%|████▎     | 29/68 [1:06:45<1:27:42, 134.93s/batch]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Processing Batches:  44%|████▍     | 30/68 [1:09:02<1:25:45, 135.41s/batch]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Processing Batches:  46%|████▌     | 31/68 [1:11:09<1:22:05, 133.13s/batch]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Processing Batches:  47%|████▋     | 32/68 [1:13:25<1:20:20, 133.89s/batch]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Processing Batches:  49%|████▊     | 33/68 [1:15:35<1:17:21, 132.60s/batch]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Processing Batches:  50%|█████     | 34/68 [1:17:41<1:14:05, 130.74s/batch]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Processing Batches:  51%|█████▏    | 35/68 [1:19:53<1:12:01, 130.95s/batch]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Processing Batches:  53%|█████▎    | 36/68 [1:22:16<1:11:54, 134.82s/batch]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Processing Batches:  54%|█████▍    | 37/68 [1:24:34<1:10:06, 135.68s/batch]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Processing Batches:  56%|█████▌    | 38/68 [1:26:51<1:08:04, 136.14s/batch]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Processing Batches:  57%|█████▋    | 39/68 [1:29:00<1:04:42, 133.89s/batch]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Processing Batches:  59%|█████▉    | 40/68 [1:31:06<1:01:25, 131.63s/batch]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Processing Batches:  60%|██████    | 41/68 [1:33:28<1:00:34, 134.62s/batch]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Processing Batches:  62%|██████▏   | 42/68 [1:35:40<57:57, 133.76s/batch]  Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Processing Batches:  63%|██████▎   | 43/68 [1:37:51<55:25, 133.02s/batch]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Processing Batches:  65%|██████▍   | 44/68 [1:39:53<51:56, 129.84s/batch]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Processing Batches:  66%|██████▌   | 45/68 [1:44:45<1:08:26, 178.53s/batch]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Processing Batches:  68%|██████▊   | 46/68 [1:46:52<59:41, 162.79s/batch]  Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Processing Batches:  69%|██████▉   | 47/68 [1:48:58<53:12, 152.02s/batch]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Processing Batches:  71%|███████   | 48/68 [1:51:11<48:41, 146.06s/batch]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Processing Batches:  72%|███████▏  | 49/68 [1:53:14<44:08, 139.37s/batch]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Processing Batches:  74%|███████▎  | 50/68 [1:55:49<43:11, 143.96s/batch]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Processing Batches:  75%|███████▌  | 51/68 [1:58:09<40:24, 142.63s/batch]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Processing Batches:  76%|███████▋  | 52/68 [2:00:34<38:17, 143.57s/batch]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Processing Batches:  78%|███████▊  | 53/68 [2:02:39<34:30, 138.03s/batch]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Processing Batches:  79%|███████▉  | 54/68 [2:04:50<31:39, 135.66s/batch]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Processing Batches:  81%|████████  | 55/68 [2:07:13<29:53, 137.95s/batch]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Processing Batches:  82%|████████▏ | 56/68 [2:09:23<27:05, 135.49s/batch]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Processing Batches:  84%|████████▍ | 57/68 [2:11:40<24:57, 136.12s/batch]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Processing Batches:  85%|████████▌ | 58/68 [2:13:53<22:31, 135.19s/batch]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Processing Batches:  87%|████████▋ | 59/68 [2:16:01<19:55, 132.86s/batch]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Processing Batches:  88%|████████▊ | 60/68 [2:18:18<17:54, 134.26s/batch]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Processing Batches:  90%|████████▉ | 61/68 [2:20:35<15:45, 135.05s/batch]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Processing Batches:  91%|█████████ | 62/68 [2:22:45<13:21, 133.52s/batch]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Processing Batches:  93%|█████████▎| 63/68 [2:25:04<11:15, 135.11s/batch]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Processing Batches:  94%|█████████▍| 64/68 [2:27:19<09:00, 135.01s/batch]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Processing Batches:  96%|█████████▌| 65/68 [2:29:32<06:43, 134.40s/batch]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Processing Batches:  97%|█████████▋| 66/68 [2:31:50<04:31, 135.67s/batch]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Processing Batches:  99%|█████████▊| 67/68 [2:34:10<02:17, 137.06s/batch]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Processing Batches: 100%|██████████| 68/68 [2:35:56<00:00, 137.59s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Responses saved to ../data/input_response_data.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "# Device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"google/IFEval\")\n",
    "\n",
    "# Model name\n",
    "model_name = \"Qwen/Qwen2.5-7B\"\n",
    "\n",
    "# Load the tokenizer for Mistral\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    add_eos_token=True,\n",
    "    use_fast=True,\n",
    "    padding_side='left'\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Set padding token to EOS token\n",
    "\n",
    "# Quantization configuration using bitsandbytes library\n",
    "# Config for 8 bit quantization\n",
    "nf8_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    bnb_8bit_quant_type=\"nf8\",\n",
    "    bnb_8bit_use_double_quant=True,\n",
    "    bnb_8bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "# Step 2: Get trained LORA and BNB model\n",
    "lora_location = \"Asteris/qwen2.5-7B-LoRa-efficient-training\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    lora_location, \n",
    "    torch_dtype=torch.float16,  # Use float16 for mixed precision training\n",
    "    device_map=device,  # Distribute the model automatically across GPUs\n",
    "    quantization_config=nf8_config,  # Use the bitsandbytes quantization config\n",
    ")\n",
    "model.config.pad_token_id = tokenizer.pad_token_id  # Set the model's padding token ID\n",
    "\n",
    "# Disable gradients to save memory and computation\n",
    "model.eval()\n",
    "torch.set_grad_enabled(False)  # Disable gradient computation globally\n",
    "\n",
    "# Prepare the output file\n",
    "output_dir = \"../data\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "output_file = os.path.join(output_dir, \"input_response_data.jsonl\")\n",
    "\n",
    "# Batch processing\n",
    "batch_size = 8  # Adjust based on your GPU memory capacity\n",
    "max_length = 512  # Limit output length to avoid excessive memory usage\n",
    "\n",
    "with open(output_file, 'w') as f:\n",
    "    # Process in batches\n",
    "    for i in tqdm(range(0, len(dataset['train']), batch_size), desc=\"Processing Batches\", unit=\"batch\"):\n",
    "        try:\n",
    "            if (i + batch_size) > len(dataset['train']):\n",
    "                batch_prompts = dataset['train']['prompt'][i:len(dataset['train'])]\n",
    "            else:\n",
    "                batch_prompts = dataset['train']['prompt'][i:i + batch_size]\n",
    "            # Tokenize inputs\n",
    "            inputs = tokenizer(batch_prompts, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "            \n",
    "            # Generate responses\n",
    "            with torch.no_grad():  # Ensure gradients are disabled during generation\n",
    "                if max_length:\n",
    "                    outputs = model.generate(**inputs, max_new_tokens=max_length)\n",
    "                else:\n",
    "                    outputs = model.generate(**inputs)\n",
    "            # Decode responses\n",
    "            responses = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "\n",
    "            # Write each response directly to the file\n",
    "            for prompt, response in zip(batch_prompts, responses):\n",
    "                f.write(json.dumps({\"prompt\": prompt, \"response\": response}) + '\\n')\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing batch {i}: {e}\")\n",
    "\n",
    "print(f\"Responses saved to {output_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
